import re

import pandas as pd
import requests
import streamlit as st

CSV_URL = "https://drive.google.com/uc?id=1Lg1xB79PYue1D5qSPy2IMo6zNMjOgEa4&export=download"


@st.cache_data(ttl=300)
def load_index_csv() -> pd.DataFrame:
    """index.csvã‚’èª­ã¿è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãŸã‚ã¦ãŠãé–¢æ•°

    Returns:
        pd.DataFrame

    """
    return pd.read_csv(CSV_URL)


def extract_file_id(drive_url: str) -> str | None:
    """Google driveã®urlã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«idã‚’å–å¾—ã™ã‚‹

    Args:
        drive_url: str
            google driveã®url

    Returns:
        str | None
            ãƒ•ã‚¡ã‚¤ãƒ«id

    """
    match = re.search(r"/d/([a-zA-Z0-9_-]+)", drive_url)
    if match:
        return match.group(1)
    return None


@st.cache_data()
def load_kif_text(kif_url: str) -> str:
    """æ£‹è­œãƒ‡ãƒ¼ã‚¿ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’urlã‹ã‚‰å–å¾—ã™ã‚‹

    Args:
        kif_url: str
            æ£‹è­œãƒ‡ãƒ¼ã‚¿ã®url

    Returns:
        str
            kifå½¢å¼ã®æ£‹è­œãƒ‡ãƒ¼ã‚¿

    """
    kif_if = extract_file_id(kif_url)
    response = requests.get(f"https://drive.google.com/uc?id={kif_if}&export=download", timeout=300)
    response.raise_for_status()
    return response.text


def get_match_text(df: pd.DataFrame, index: int) -> str:
    """æŒ‡å®šã•ã‚ŒãŸindexã®å¯¾å±€æƒ…å ±ã‚’ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å—ã‘å–ã‚‹

    Args:
        df: pd.DataFrame
            å¯¾å±€æƒ…å ±ã‚’æ ¼ç´ã—ãŸãƒ†ãƒ¼ãƒ–ãƒ«
        index: int
            å¯¾å±€æƒ…å ±ã‚’è¿”ã—ãŸã„ãƒ†ãƒ¼ãƒ–ãƒ«ã®index

    Returns:
        str: _description_

    """
    date = df.loc[index, "æ—¥ä»˜"]
    no = df.loc[index, "è©¦åˆç•ªå·"]
    sente = df.loc[index, "å…ˆæ‰‹"]
    gote = df.loc[index, "å¾Œæ‰‹"]
    teaiwari = df.loc[index, "æ‰‹åˆå‰²"]
    result = df.loc[index, "çµæœ"]
    return f"{date} {no}å±€ç›® {sente} å¯¾ {gote} ({teaiwari}, {result})"


st.set_page_config(page_title="ãƒˆãƒ¢ã‚·ã‚«å¯¾å±€æ£‹è­œãƒ‡ãƒ¼ã‚¿", page_icon="ğŸ¦Œ")

st.title("â˜–ãƒˆãƒ¢ã‚·ã‚«å¯¾å±€æ£‹è­œãƒ‡ãƒ¼ã‚¿ğŸ¦Œ")

df = load_index_csv()


col = st.columns([4, 1])

selected_index = st.selectbox(
    "å¯¾å±€ã‚’é¸æŠ",
    sorted(df.index, reverse=True),
    format_func=lambda i: get_match_text(df, i),
)

row = df.loc[selected_index]

st.markdown(f"""
- æ—¥ä»˜ã€€: {row["æ—¥ä»˜"]} - {row["è©¦åˆç•ªå·"]}å±€ç›®
- å…ˆæ‰‹ã€€: {row["å…ˆæ‰‹"]}
- å¾Œæ‰‹ã€€: {row["å¾Œæ‰‹"]}
- æ‰‹åˆå‰²: {row["æ‰‹åˆå‰²"]}
- çµæœã€€: {row["çµæœ"]}
- å‹•ç”»ã€€: [ã“ã¡ã‚‰ã®ãƒªãƒ³ã‚¯]({row["å‹•ç”»URL"]})
""")

try:
    kif_text = load_kif_text(row["æ£‹è­œãƒ‡ãƒ¼ã‚¿URL"])
    st.text_area("æ£‹è­œï¼ˆKIFå½¢å¼ï¼‰", kif_text, height=300)
    st.download_button(
        "æ£‹è­œã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        kif_text,
        file_name=get_match_text(df, selected_index) + ".txt",
        mime="text/plain",
        type="primary",
    )
except FileNotFoundError as e:
    st.error(f"æ£‹è­œã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


st.divider()
st.markdown(
    "[ç·‹ç¬ ãƒˆãƒ¢ã‚·ã‚«](https://www.youtube.com/@tomoshikahikasa)ã•ã‚“ã‚„ãã®ãŠå‹é”ã®æ–¹ãŒå¯¾å±€ã—ãŸæ£‹è­œãƒ‡ãƒ¼ã‚¿ã‚’ã¾ã¨ã‚ã¦ã„ã¾ã™ã€‚æ£‹è­œã¯æ‰‹å…¥åŠ›ã§ã™ã®ã§ãƒŸã‚¹ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€ã”äº†æ‰¿ãã ã•ã„ã€‚"
)

st.markdown("ã“ã®ã‚µã‚¤ãƒˆã«ã¤ã„ã¦ã®å•ã„åˆã‚ã›ã‚„æ£‹è­œã®ãƒŸã‚¹ã«ã¤ã„ã¦ã¯[@mega_ebi](https://x.com/mega_ebi)ã¾ã§")

with st.expander("ã“ã®ãƒšãƒ¼ã‚¸ã«é–¢ã™ã‚‹ãã®ä»–ã®æ³¨è¨˜"):
    st.markdown(
        "2024/08/12 - [ã€å°†æ£‹ã€‘VOMSå°†æ£‹éƒ¨ã§ãã¡ã‚ƒãã¡ã‚ƒ10ç§’å°†æ£‹ç·å½“ãŸã‚Šå¤§ä¼šï¼ï¼](https://www.youtube.com/live/Y7jC8mEBUbE?si=HIPig2pgMW3q4DeI)ã§å®Ÿæ–½ã—ãŸå¯¾å±€ã¯ã€ã‚ã¾ã‚Šã«ã‚‚å¯¾å±€æ•°ãŒå¤šãæ‰‹å…¥åŠ›ã®æ‰‹é–“ãŒã‹ã‹ã‚‹ãŸã‚ã€å°‘ãªãã¨ã‚‚å½“é¢ã¯å¯¾è±¡å¤–ã§ã™ã€‚"
    )
    st.markdown("2025/08/12 - ï¼•äº”å°†æ£‹ã€çŸ­æ™‚é–“ã§ã®åˆ‡ã‚Œè² ã‘å°†æ£‹ã¯åŸºæœ¬çš„ã«å¯¾è±¡å¤–ã§ã™ã€‚")
